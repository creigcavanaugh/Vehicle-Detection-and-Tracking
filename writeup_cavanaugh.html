<!DOCTYPE html><html><head><meta charset="utf-8"><style>@font-face {
  font-family: octicons-anchor;
  src: url(data:font/woff;charset=utf-8;base64,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) format('woff');
}

* {
    box-sizing: border-box;
}

body {
    width: 980px;
    margin-right: auto;
    margin-left: auto;
}

body .markdown-body {
    padding: 45px;
    border: 1px solid #ddd;
    border-radius: 3px;
    word-wrap: break-word;
}

pre {
    font: 12px Consolas, "Liberation Mono", Menlo, Courier, monospace;
}

.markdown-body {
  -webkit-text-size-adjust: 100%;
  text-size-adjust: 100%;
  color: #333;
  font-family: "Helvetica Neue", Helvetica, "Segoe UI", Arial, freesans, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
  font-size: 16px;
  line-height: 1.6;
  word-wrap: break-word;
}

.markdown-body a {
  background-color: transparent;
}

.markdown-body a:active,
.markdown-body a:hover {
  outline: 0;
}

.markdown-body strong {
  font-weight: bold;
}

.markdown-body h1 {
  font-size: 2em;
  margin: 0.67em 0;
}

.markdown-body img {
  border: 0;
}

.markdown-body hr {
  box-sizing: content-box;
  height: 0;
}

.markdown-body pre {
  overflow: auto;
}

.markdown-body code,
.markdown-body kbd,
.markdown-body pre {
  font-family: monospace, monospace;
  font-size: 1em;
}

.markdown-body input {
  color: inherit;
  font: inherit;
  margin: 0;
}

.markdown-body html input[disabled] {
  cursor: default;
}

.markdown-body input {
  line-height: normal;
}

.markdown-body input[type="checkbox"] {
  box-sizing: border-box;
  padding: 0;
}

.markdown-body table {
  border-collapse: collapse;
  border-spacing: 0;
}

.markdown-body td,
.markdown-body th {
  padding: 0;
}

.markdown-body input {
  font: 13px / 1.4 Helvetica, arial, nimbussansl, liberationsans, freesans, clean, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
}

.markdown-body a {
  color: #4078c0;
  text-decoration: none;
}

.markdown-body a:hover,
.markdown-body a:active {
  text-decoration: underline;
}

.markdown-body hr {
  height: 0;
  margin: 15px 0;
  overflow: hidden;
  background: transparent;
  border: 0;
  border-bottom: 1px solid #ddd;
}

.markdown-body hr:before {
  display: table;
  content: "";
}

.markdown-body hr:after {
  display: table;
  clear: both;
  content: "";
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  margin-top: 15px;
  margin-bottom: 15px;
  line-height: 1.1;
}

.markdown-body h1 {
  font-size: 30px;
}

.markdown-body h2 {
  font-size: 21px;
}

.markdown-body h3 {
  font-size: 16px;
}

.markdown-body h4 {
  font-size: 14px;
}

.markdown-body h5 {
  font-size: 12px;
}

.markdown-body h6 {
  font-size: 11px;
}

.markdown-body blockquote {
  margin: 0;
}

.markdown-body ul,
.markdown-body ol {
  padding: 0;
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body ol ol,
.markdown-body ul ol {
  list-style-type: lower-roman;
}

.markdown-body ul ul ol,
.markdown-body ul ol ol,
.markdown-body ol ul ol,
.markdown-body ol ol ol {
  list-style-type: lower-alpha;
}

.markdown-body dd {
  margin-left: 0;
}

.markdown-body code {
  font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
  font-size: 12px;
}

.markdown-body pre {
  margin-top: 0;
  margin-bottom: 0;
  font: 12px Consolas, "Liberation Mono", Menlo, Courier, monospace;
}

.markdown-body .select::-ms-expand {
  opacity: 0;
}

.markdown-body .octicon {
  font: normal normal normal 16px/1 octicons-anchor;
  display: inline-block;
  text-decoration: none;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.markdown-body .octicon-link:before {
  content: '\f05c';
}

.markdown-body:before {
  display: table;
  content: "";
}

.markdown-body:after {
  display: table;
  clear: both;
  content: "";
}

.markdown-body>*:first-child {
  margin-top: 0 !important;
}

.markdown-body>*:last-child {
  margin-bottom: 0 !important;
}

.markdown-body a:not([href]) {
  color: inherit;
  text-decoration: none;
}

.markdown-body .anchor {
  display: inline-block;
  padding-right: 2px;
  margin-left: -18px;
}

.markdown-body .anchor:focus {
  outline: none;
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  margin-top: 1em;
  margin-bottom: 16px;
  font-weight: bold;
  line-height: 1.4;
}

.markdown-body h1 .octicon-link,
.markdown-body h2 .octicon-link,
.markdown-body h3 .octicon-link,
.markdown-body h4 .octicon-link,
.markdown-body h5 .octicon-link,
.markdown-body h6 .octicon-link {
  color: #000;
  vertical-align: middle;
  visibility: hidden;
}

.markdown-body h1:hover .anchor,
.markdown-body h2:hover .anchor,
.markdown-body h3:hover .anchor,
.markdown-body h4:hover .anchor,
.markdown-body h5:hover .anchor,
.markdown-body h6:hover .anchor {
  text-decoration: none;
}

.markdown-body h1:hover .anchor .octicon-link,
.markdown-body h2:hover .anchor .octicon-link,
.markdown-body h3:hover .anchor .octicon-link,
.markdown-body h4:hover .anchor .octicon-link,
.markdown-body h5:hover .anchor .octicon-link,
.markdown-body h6:hover .anchor .octicon-link {
  visibility: visible;
}

.markdown-body h1 {
  padding-bottom: 0.3em;
  font-size: 2.25em;
  line-height: 1.2;
  border-bottom: 1px solid #eee;
}

.markdown-body h1 .anchor {
  line-height: 1;
}

.markdown-body h2 {
  padding-bottom: 0.3em;
  font-size: 1.75em;
  line-height: 1.225;
  border-bottom: 1px solid #eee;
}

.markdown-body h2 .anchor {
  line-height: 1;
}

.markdown-body h3 {
  font-size: 1.5em;
  line-height: 1.43;
}

.markdown-body h3 .anchor {
  line-height: 1.2;
}

.markdown-body h4 {
  font-size: 1.25em;
}

.markdown-body h4 .anchor {
  line-height: 1.2;
}

.markdown-body h5 {
  font-size: 1em;
}

.markdown-body h5 .anchor {
  line-height: 1.1;
}

.markdown-body h6 {
  font-size: 1em;
  color: #777;
}

.markdown-body h6 .anchor {
  line-height: 1.1;
}

.markdown-body p,
.markdown-body blockquote,
.markdown-body ul,
.markdown-body ol,
.markdown-body dl,
.markdown-body table,
.markdown-body pre {
  margin-top: 0;
  margin-bottom: 16px;
}

.markdown-body hr {
  height: 4px;
  padding: 0;
  margin: 16px 0;
  background-color: #e7e7e7;
  border: 0 none;
}

.markdown-body ul,
.markdown-body ol {
  padding-left: 2em;
}

.markdown-body ul ul,
.markdown-body ul ol,
.markdown-body ol ol,
.markdown-body ol ul {
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body li>p {
  margin-top: 16px;
}

.markdown-body dl {
  padding: 0;
}

.markdown-body dl dt {
  padding: 0;
  margin-top: 16px;
  font-size: 1em;
  font-style: italic;
  font-weight: bold;
}

.markdown-body dl dd {
  padding: 0 16px;
  margin-bottom: 16px;
}

.markdown-body blockquote {
  padding: 0 15px;
  color: #777;
  border-left: 4px solid #ddd;
}

.markdown-body blockquote>:first-child {
  margin-top: 0;
}

.markdown-body blockquote>:last-child {
  margin-bottom: 0;
}

.markdown-body table {
  display: block;
  width: 100%;
  overflow: auto;
  word-break: normal;
  word-break: keep-all;
}

.markdown-body table th {
  font-weight: bold;
}

.markdown-body table th,
.markdown-body table td {
  padding: 6px 13px;
  border: 1px solid #ddd;
}

.markdown-body table tr {
  background-color: #fff;
  border-top: 1px solid #ccc;
}

.markdown-body table tr:nth-child(2n) {
  background-color: #f8f8f8;
}

.markdown-body img {
  max-width: 100%;
  box-sizing: content-box;
  background-color: #fff;
}

.markdown-body code {
  padding: 0;
  padding-top: 0.2em;
  padding-bottom: 0.2em;
  margin: 0;
  font-size: 85%;
  background-color: rgba(0,0,0,0.04);
  border-radius: 3px;
}

.markdown-body code:before,
.markdown-body code:after {
  letter-spacing: -0.2em;
  content: "\00a0";
}

.markdown-body pre>code {
  padding: 0;
  margin: 0;
  font-size: 100%;
  word-break: normal;
  white-space: pre;
  background: transparent;
  border: 0;
}

.markdown-body .highlight {
  margin-bottom: 16px;
}

.markdown-body .highlight pre,
.markdown-body pre {
  padding: 16px;
  overflow: auto;
  font-size: 85%;
  line-height: 1.45;
  background-color: #f7f7f7;
  border-radius: 3px;
}

.markdown-body .highlight pre {
  margin-bottom: 0;
  word-break: normal;
}

.markdown-body pre {
  word-wrap: normal;
}

.markdown-body pre code {
  display: inline;
  max-width: initial;
  padding: 0;
  margin: 0;
  overflow: initial;
  line-height: inherit;
  word-wrap: normal;
  background-color: transparent;
  border: 0;
}

.markdown-body pre code:before,
.markdown-body pre code:after {
  content: normal;
}

.markdown-body kbd {
  display: inline-block;
  padding: 3px 5px;
  font-size: 11px;
  line-height: 10px;
  color: #555;
  vertical-align: middle;
  background-color: #fcfcfc;
  border: solid 1px #ccc;
  border-bottom-color: #bbb;
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 #bbb;
}

.markdown-body .pl-c {
  color: #969896;
}

.markdown-body .pl-c1,
.markdown-body .pl-s .pl-v {
  color: #0086b3;
}

.markdown-body .pl-e,
.markdown-body .pl-en {
  color: #795da3;
}

.markdown-body .pl-s .pl-s1,
.markdown-body .pl-smi {
  color: #333;
}

.markdown-body .pl-ent {
  color: #63a35c;
}

.markdown-body .pl-k {
  color: #a71d5d;
}

.markdown-body .pl-pds,
.markdown-body .pl-s,
.markdown-body .pl-s .pl-pse .pl-s1,
.markdown-body .pl-sr,
.markdown-body .pl-sr .pl-cce,
.markdown-body .pl-sr .pl-sra,
.markdown-body .pl-sr .pl-sre {
  color: #183691;
}

.markdown-body .pl-v {
  color: #ed6a43;
}

.markdown-body .pl-id {
  color: #b52a1d;
}

.markdown-body .pl-ii {
  background-color: #b52a1d;
  color: #f8f8f8;
}

.markdown-body .pl-sr .pl-cce {
  color: #63a35c;
  font-weight: bold;
}

.markdown-body .pl-ml {
  color: #693a17;
}

.markdown-body .pl-mh,
.markdown-body .pl-mh .pl-en,
.markdown-body .pl-ms {
  color: #1d3e81;
  font-weight: bold;
}

.markdown-body .pl-mq {
  color: #008080;
}

.markdown-body .pl-mi {
  color: #333;
  font-style: italic;
}

.markdown-body .pl-mb {
  color: #333;
  font-weight: bold;
}

.markdown-body .pl-md {
  background-color: #ffecec;
  color: #bd2c00;
}

.markdown-body .pl-mi1 {
  background-color: #eaffea;
  color: #55a532;
}

.markdown-body .pl-mdr {
  color: #795da3;
  font-weight: bold;
}

.markdown-body .pl-mo {
  color: #1d3e81;
}

.markdown-body kbd {
  display: inline-block;
  padding: 3px 5px;
  font: 11px Consolas, "Liberation Mono", Menlo, Courier, monospace;
  line-height: 10px;
  color: #555;
  vertical-align: middle;
  background-color: #fcfcfc;
  border: solid 1px #ccc;
  border-bottom-color: #bbb;
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 #bbb;
}

.markdown-body .plan-price-unit {
  color: #767676;
  font-weight: normal;
}

.markdown-body .task-list-item {
  list-style-type: none;
}

.markdown-body .task-list-item+.task-list-item {
  margin-top: 3px;
}

.markdown-body .task-list-item input {
  margin: 0 0.35em 0.25em -1.6em;
  vertical-align: middle;
}

.markdown-body .plan-choice {
  padding: 15px;
  padding-left: 40px;
  display: block;
  border: 1px solid #e0e0e0;
  position: relative;
  font-weight: normal;
  background-color: #fafafa;
}

.markdown-body .plan-choice.open {
  background-color: #fff;
}

.markdown-body .plan-choice.open .plan-choice-seat-breakdown {
  display: block;
}

.markdown-body .plan-choice-free {
  border-radius: 3px 3px 0 0;
}

.markdown-body .plan-choice-paid {
  border-radius: 0 0 3px 3px;
  border-top: 0;
  margin-bottom: 20px;
}

.markdown-body .plan-choice-radio {
  position: absolute;
  left: 15px;
  top: 18px;
}

.markdown-body .plan-choice-exp {
  color: #999;
  font-size: 12px;
  margin-top: 5px;
}

.markdown-body .plan-choice-seat-breakdown {
  margin-top: 10px;
  display: none;
}

.markdown-body :checked+.radio-label {
  z-index: 1;
  position: relative;
  border-color: #4078c0;
}
</style><title>writeup_cavanaugh</title></head><body><article class="markdown-body"><h2>
<a id="user-content-vehicle-detection-project" class="anchor" href="#vehicle-detection-project" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Vehicle Detection Project</h2>
<p><strong>Creig Cavanaugh - April 2017</strong></p>
<p>The goals / steps of this project are the following:</p>
<ul>
<li>Perform a Histogram of Oriented Gradients (HOG) feature extraction on a labeled training set of images and train a classifier Linear SVM classifier</li>
<li>Optionally, you can also apply a color transform and append binned color features, as well as histograms of color, to your HOG feature vector.</li>
<li>Note: for those first two steps don't forget to normalize your features and randomize a selection for training and testing.</li>
<li>Implement a sliding-window technique and use your trained classifier to search for vehicles in images.</li>
<li>Run your pipeline on a video stream (start with the test_video.mp4 and later implement on full project_video.mp4) and create a heat map of recurring detections frame by frame to reject outliers and follow detected vehicles.</li>
<li>Estimate a bounding box for vehicles detected.</li>
</ul>
<h2>
<a id="user-content-rubric-points" class="anchor" href="#rubric-points" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://review.udacity.com/#!/rubrics/513/view">Rubric</a> Points</h2>
<h3>
<a id="user-content-here-i-will-consider-the-rubric-points-individually-and-describe-how-i-addressed-each-point-in-my-implementation" class="anchor" href="#here-i-will-consider-the-rubric-points-individually-and-describe-how-i-addressed-each-point-in-my-implementation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Here I will consider the rubric points individually and describe how I addressed each point in my implementation.</h3>
<hr>
<h3>
<a id="user-content-writeup--readme" class="anchor" href="#writeup--readme" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Writeup / README</h3>
<h4>
<a id="user-content-1-provide-a-writeup--readme-that-includes-all-the-rubric-points-and-how-you-addressed-each-one--you-can-submit-your-writeup-as-markdown-or-pdf--here-is-a-template-writeup-for-this-project-you-can-use-as-a-guide-and-a-starting-point" class="anchor" href="#1-provide-a-writeup--readme-that-includes-all-the-rubric-points-and-how-you-addressed-each-one--you-can-submit-your-writeup-as-markdown-or-pdf--here-is-a-template-writeup-for-this-project-you-can-use-as-a-guide-and-a-starting-point" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>1. Provide a Writeup / README that includes all the rubric points and how you addressed each one.  You can submit your writeup as markdown or pdf.  <a href="https://github.com/udacity/CarND-Vehicle-Detection/blob/master/writeup_template.md">Here</a> is a template writeup for this project you can use as a guide and a starting point.</h4>
<p>You're reading it!</p>
<h3>
<a id="user-content-histogram-of-oriented-gradients-hog" class="anchor" href="#histogram-of-oriented-gradients-hog" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Histogram of Oriented Gradients (HOG)</h3>
<h4>
<a id="user-content-1-explain-how-and-identify-where-in-your-code-you-extracted-hog-features-from-the-training-images" class="anchor" href="#1-explain-how-and-identify-where-in-your-code-you-extracted-hog-features-from-the-training-images" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>1. Explain how (and identify where in your code) you extracted HOG features from the training images.</h4>
<p>The code for this step is contained in lines 226 through 235 of the file called <code>vehicle_detection.py</code>, which calls the <code>extract_features</code> function
in the module <code>vdlib.py</code>.</p>
<p>I started by reading in all the <code>vehicle</code> and <code>non-vehicle</code> images.  Here is an example of one of each of the <code>vehicle</code> and <code>non-vehicle</code> classes:</p>
<p><a href="/home/creigc/Dropbox/Udacity/self_driving_car/Vehicle%20Detection/P5_Cavanaugh/output_images/car_notcar.png" target="_blank"><img src="/home/creigc/Dropbox/Udacity/self_driving_car/Vehicle%20Detection/P5_Cavanaugh/output_images/car_notcar.png" alt="alt text" style="max-width:100%;"></a></p>
<p>I then explored different color spaces and different <code>skimage.hog()</code> parameters (<code>orientations</code>, <code>pixels_per_cell</code>, and <code>cells_per_block</code>).  I grabbed random images from each of the two classes and displayed them to get a feel for what the <code>skimage.hog()</code> output looks like.</p>
<p>Here is an example using the <code>RGB</code> color space and HOG parameters of <code>orientations=12</code>, <code>pixels_per_cell=(8, 8)</code> and <code>cells_per_block=(2, 2)</code>:</p>
<p><a href="/home/creigc/Dropbox/Udacity/self_driving_car/Vehicle%20Detection/P5_Cavanaugh/output_images/hog_channels.png" target="_blank"><img src="/home/creigc/Dropbox/Udacity/self_driving_car/Vehicle%20Detection/P5_Cavanaugh/output_images/hog_channels.png" alt="alt text" style="max-width:100%;"></a></p>
<h4>
<a id="user-content-2-explain-how-you-settled-on-your-final-choice-of-hog-parameters" class="anchor" href="#2-explain-how-you-settled-on-your-final-choice-of-hog-parameters" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>2. Explain how you settled on your final choice of HOG parameters.</h4>
<p>I tried various combinations of parameters  - I seemed to have the best results staying with 9 pixels per cell, and two cells per block.  I experimented with all the color spaces, and interestingly RGB works the best with the pipeline I setup, which uses all color channels for HOG.</p>
<p>I also experimented with using different HOG orientations - I got good results with 7 orientations using the SVM linear kernel, but switched to the RBF kernel and ended up using 12.</p>
<p>I also included both Spatial and Color Histogram into the feature set.  I experimented initially with just using the HOG features, but got more robust results when adding in the additional features.</p>
<p><a href="/home/creigc/Dropbox/Udacity/self_driving_car/Vehicle%20Detection/P5_Cavanaugh/output_images/spatial_color_histogram.png" target="_blank"><img src="/home/creigc/Dropbox/Udacity/self_driving_car/Vehicle%20Detection/P5_Cavanaugh/output_images/spatial_color_histogram.png" alt="alt text" style="max-width:100%;"></a></p>
<p>Here are my final parameters:</p>
<pre lang="color_space"><code>spatial = 16
histbin = 32
orient = 12  # HOG orientations
pix_per_cell = 8 # HOG pixels per cell
cell_per_block = 2 # HOG cells per block
hog_channel = "ALL" # Can be 0, 1, 2, or "ALL"
</code></pre>
<h4>
<a id="user-content-3-describe-how-and-identify-where-in-your-code-you-trained-a-classifier-using-your-selected-hog-features-and-color-features-if-you-used-them" class="anchor" href="#3-describe-how-and-identify-where-in-your-code-you-trained-a-classifier-using-your-selected-hog-features-and-color-features-if-you-used-them" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>3. Describe how (and identify where in your code) you trained a classifier using your selected HOG features (and color features if you used them).</h4>
<p>I created a function called <code>get_image_set</code> in the <code>vdlib.pl</code> module that provides randomly shuffled image filenames for use in training the classifier.  I ended up using 5000 vehicle and non-vehicle images to train the classifier.  The function is called on line 222 in my code in <code>vehicle_detection.py</code>.</p>
<p>I used the <code>extract_features</code> function in the <code>vdlib.pl</code> module to extract the HOG and color features from the training data.  Features were labeled 1=car and 0=not-car.  The features were normalized using the <code>StandardScaler()</code> from the <code>sklearn.preprocessing</code> package.  On line 252 in <code>vehicle_detection.py</code>, the data set is randomized and split between training and test data (80% / 20%).</p>
<p>I used GridSearchCV to determine optimized hyper-parameters, using the following command:</p>
<pre><code>from sklearn.grid_search import GridSearchCV
from sklearn.svm import SVC
parameters = {'kernel':('linear', 'rbf'), 'C':[0.1, 0.5, 1, 5, 10, 100],'gamma': [0.001, 0.0001]}
svr = SVC()
clf = GridSearchCV(svr, parameters)
clf.fit(X_train, y_train)
print (clf.best_params_)

</code></pre>
<p>The output of GridSearch indicated RBF is the optimal kernel, using C=1.0 and Gamma=0.0001.  The SVM training is done on lines 276 through 289 in <code>vehicle_detection.py</code>.</p>
<h3>
<a id="user-content-sliding-window-search" class="anchor" href="#sliding-window-search" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Sliding Window Search</h3>
<h4>
<a id="user-content-1-describe-how-and-identify-where-in-your-code-you-implemented-a-sliding-window-search--how-did-you-decide-what-scales-to-search-and-how-much-to-overlap-windows" class="anchor" href="#1-describe-how-and-identify-where-in-your-code-you-implemented-a-sliding-window-search--how-did-you-decide-what-scales-to-search-and-how-much-to-overlap-windows" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>1. Describe how (and identify where in your code) you implemented a sliding window search.  How did you decide what scales to search and how much to overlap windows?</h4>
<p>I implemented this step in lines 56 through 130 in my code in <code>vehicle_detection.py</code> in the function called <code>find_cars</code>.  I utilized the sliding window approach as described in the 'Hog Sub-sampling Window Search' section of the Vehicle Detection and Tracking lesson. I limited the HOG search window, using ystart = 400 and ystop = 656. I experimented with different scales to find the best detection performance, and ended up calling the function three times to search at the following scales: 1.5, 2.5 and 3.0</p>
<p>Here are examples of the grid pattern for each:</p>
<p><strong>1.5 Scale</strong>
<a href="/home/creigc/Dropbox/Udacity/self_driving_car/Vehicle%20Detection/P5_Cavanaugh/output_images/scale_15.png" target="_blank"><img src="/home/creigc/Dropbox/Udacity/self_driving_car/Vehicle%20Detection/P5_Cavanaugh/output_images/scale_15.png" alt="alt text" style="max-width:100%;"></a></p>
<p><strong>2.5 Scale</strong>
<a href="/home/creigc/Dropbox/Udacity/self_driving_car/Vehicle%20Detection/P5_Cavanaugh/output_images/scale_25.png" target="_blank"><img src="/home/creigc/Dropbox/Udacity/self_driving_car/Vehicle%20Detection/P5_Cavanaugh/output_images/scale_25.png" alt="alt text" style="max-width:100%;"></a></p>
<p><strong>3.0 Scale</strong>
<a href="/home/creigc/Dropbox/Udacity/self_driving_car/Vehicle%20Detection/P5_Cavanaugh/output_images/scale_30.png" target="_blank"><img src="/home/creigc/Dropbox/Udacity/self_driving_car/Vehicle%20Detection/P5_Cavanaugh/output_images/scale_30.png" alt="alt text" style="max-width:100%;"></a></p>
<h4>
<a id="user-content-2-show-some-examples-of-test-images-to-demonstrate-how-your-pipeline-is-working--what-did-you-do-to-optimize-the-performance-of-your-classifier" class="anchor" href="#2-show-some-examples-of-test-images-to-demonstrate-how-your-pipeline-is-working--what-did-you-do-to-optimize-the-performance-of-your-classifier" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>2. Show some examples of test images to demonstrate how your pipeline is working.  What did you do to optimize the performance of your classifier?</h4>
<p>Ultimately I searched on three scales using RGB 3-channel HOG features plus spatially binned color and histograms of color in the feature vector, which provided a nice result.  Here are some example images:</p>
<p>Here is an early result, which does not pick up both cars:
<a href="/home/creigc/Dropbox/Udacity/self_driving_car/Vehicle%20Detection/P5_Cavanaugh/output_images/not_detecting_side_car.png" target="_blank"><img src="/home/creigc/Dropbox/Udacity/self_driving_car/Vehicle%20Detection/P5_Cavanaugh/output_images/not_detecting_side_car.png" alt="alt text" style="max-width:100%;"></a></p>
<p>Here is an improved result:</p>
<p><a href="/home/creigc/Dropbox/Udacity/self_driving_car/Vehicle%20Detection/P5_Cavanaugh/output_images/Window_Search_example.png" target="_blank"><img src="/home/creigc/Dropbox/Udacity/self_driving_car/Vehicle%20Detection/P5_Cavanaugh/output_images/Window_Search_example.png" alt="alt text" style="max-width:100%;"></a></p>
<h3>
<a id="user-content-video-implementation" class="anchor" href="#video-implementation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Video Implementation</h3>
<h4>
<a id="user-content-1-provide-a-link-to-your-final-video-output--your-pipeline-should-perform-reasonably-well-on-the-entire-project-video-somewhat-wobbly-or-unstable-bounding-boxes-are-ok-as-long-as-you-are-identifying-the-vehicles-most-of-the-time-with-minimal-false-positives" class="anchor" href="#1-provide-a-link-to-your-final-video-output--your-pipeline-should-perform-reasonably-well-on-the-entire-project-video-somewhat-wobbly-or-unstable-bounding-boxes-are-ok-as-long-as-you-are-identifying-the-vehicles-most-of-the-time-with-minimal-false-positives" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>1. Provide a link to your final video output.  Your pipeline should perform reasonably well on the entire project video (somewhat wobbly or unstable bounding boxes are ok as long as you are identifying the vehicles most of the time with minimal false positives.)</h4>
<p>Here's a <a href="/home/creigc/Dropbox/Udacity/self_driving_car/Vehicle%20Detection/P5_Cavanaugh/video_output.mp4">link to my video result</a></p>
<h4>
<a id="user-content-2-describe-how-and-identify-where-in-your-code-you-implemented-some-kind-of-filter-for-false-positives-and-some-method-for-combining-overlapping-bounding-boxes" class="anchor" href="#2-describe-how-and-identify-where-in-your-code-you-implemented-some-kind-of-filter-for-false-positives-and-some-method-for-combining-overlapping-bounding-boxes" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>2. Describe how (and identify where in your code) you implemented some kind of filter for false positives and some method for combining overlapping bounding boxes.</h4>
<p>I implemented this step in lines 134 through 191 in my code in <code>vehicle_detection.py</code>.</p>
<p>I recorded the positions of positive detections in each frame of the video.  From the positive detections I created a heatmap and then thresholded that map to identify vehicle positions.  I then used <code>scipy.ndimage.measurements.label()</code> to identify individual blobs in the heatmap.  I then assumed each blob corresponded to a vehicle.  I constructed bounding boxes to cover the area of each blob detected.</p>
<p><a href="/home/creigc/Dropbox/Udacity/self_driving_car/Vehicle%20Detection/P5_Cavanaugh/output_images/heat_map_car_positions_example.png" target="_blank"><img src="/home/creigc/Dropbox/Udacity/self_driving_car/Vehicle%20Detection/P5_Cavanaugh/output_images/heat_map_car_positions_example.png" alt="alt text" style="max-width:100%;"></a></p>
<p>In addition, I added the positions of positive detections to an array that holds the previous 7 video frames, and that compilation of detections is used to generate the heatmap, which helps to further reduce false positives.  Because of this, I use a heat threshold of 6.</p>
<p>The video result provides a active view of the heat map in the upper right of the video.</p>
<p><a href="/home/creigc/Dropbox/Udacity/self_driving_car/Vehicle%20Detection/P5_Cavanaugh/output_images/video_preview.png" target="_blank"><img src="/home/creigc/Dropbox/Udacity/self_driving_car/Vehicle%20Detection/P5_Cavanaugh/output_images/video_preview.png" alt="alt text" style="max-width:100%;"></a></p>
<hr>
<h3>
<a id="user-content-discussion" class="anchor" href="#discussion" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Discussion</h3>
<h4>
<a id="user-content-1-briefly-discuss-any-problems--issues-you-faced-in-your-implementation-of-this-project--where-will-your-pipeline-likely-fail--what-could-you-do-to-make-it-more-robust" class="anchor" href="#1-briefly-discuss-any-problems--issues-you-faced-in-your-implementation-of-this-project--where-will-your-pipeline-likely-fail--what-could-you-do-to-make-it-more-robust" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>1. Briefly discuss any problems / issues you faced in your implementation of this project.  Where will your pipeline likely fail?  What could you do to make it more robust?</h4>
<p>Here I'll talk about the approach I took, what techniques I used, what worked and why, where the pipeline might fail and how I might improve it if I were going to pursue this project further.</p>
<p>I had some initial difficulty with importing jpg vs png files, and the nuances of handling them differently in the code.  Similarly, need to be careful with the RGB vs BGR differences.</p>
<p>I initially used the linear kernel in SVM, and switched to the RBF kernel after using GridSearch and finding RBF performed the best.  I noticed I had to re-tune some of my other parameters after the switch to RBF, and also noticed processing took longer when implementing RBF.  A future improvement could be to improve and simplify the extracted features - this could minimize the performance difference between the two kernels, and make it practical to possibly switch back to linear SVM for quicker processing times.</p>
<p>The data sets used were of 64x64 images of vehicles and non-vehicles.  Many of the images in the dataset appear to be smaller hatchback type cars, which is not exactly representative of vehicles in the US. Additional types of images, such as broader categories of vehicles and more side views could help performance.  I would also further experiment using higher resolution data-set of vehicles to see if that has any impact to vehicle detection performance.</p>
<p>During the beginning of the tuning process, I noticed there were a lot of false positives when other structural components were in view, specifically structures like the guard rail on the bridges.  This could be an indication the pipeline could fail in more urban settings with structures such as buildings, fences, signs, bridges and tunnels. To possibly help, more of these types of images could be added to the "non-vehicle" image set.</p>
<p>In addition, night-time and adverse weather conditions would probably greatly reduce vehicle detection performance in its current state.  A future improvement could be to develop a classifier tuned for nighttime conditions or bad weather.</p>
</article></body></html>